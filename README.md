# COMA

## Introduction

COMA is a collaborative multi-role agent framework for automatic high-quality lesson plans generation. COMA formulates lesson plan generation as a structured workflow among three pedagogical agents: (1) the novice agent, representing a novice teacher, drafts the initial lesson plan by outlining an overarching lesson flow, though with limited precision in specifying instructional actions; (2) the veteran agent, representing an experienced teacher, enriches the draft with domain-specific knowledge by leveraging Retrieval-Augmented Generation (RAG) to retrieve authoritative content from curriculum-aligned textbooks; and (3) the master agent, representing a pedagogical expert, refines the draft through error detection, adaptation, and knowledge mapping, ensuring instructional coherence and responsiveness. An iterative workflow coordinates these agents, allowing the generated lesson plan to be progressively improved until a high-quality, deployable plan is produced, which simulates the collaborative and iterative lesson preparation process in real-world teaching practice. We evaluate COMA on lesson plan generation tasks across five subjects: mathematics, English, chemistry, science, and information technology. To ensure rigor and fairness, subject-matter educational experts designed evaluation metrics tailored to the pedagogical requirements of each subject. Experimental results demonstrate that COMA consistently outperforms state-of-the-art methods, producing lesson plans with superior quality, coherence, and pedagogical alignment.

![1739264330525](image/README/framework.png)

## Running the Code

### The results of baselines

The results generated by each baseline are saved in `./code/result/subject_results`. Each `.xlsx` file represents a subject and stores the basic information of the results: `Subject, Baseline, Id, Course Information, Lesson Plan Content`.

### Consistent

We use the $Kappa$ index as an indicator of scoring consistency calculation to verify the validity of the scoring. The scoring details of the baseline generation results are saved in `./code/result/consistent`, which has five folders corresponding to five subjects. Each subject saves the baseline results independently, such as `ChatGPT4prompt_chemistry`. Each `.xlsx` file saves the results of three scorers. The first column is the ID, and the following are the scoring results of different dimensions. For the convenience of calculation, only the scoring results are retained. Finally, the consistency of each subject is saved in `./code/result/result_consistent.csv`, the final result is saved in `./code/result/result_consistent.csv`, and the scoring consistency details are saved in `./code/result`.

#### Code

```
cd code
pip install -r requirements.txt
python consistent.py
```

## Prompt

There are many prompt words used in LessonPlan-Agent. Some of them are now saved in `./code/prompt`.
